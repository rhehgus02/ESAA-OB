{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhehgus02/ESAA_OB/blob/main/%ED%8C%8C%EC%9D%B4%EC%8D%AC_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%99%84%EB%B2%BD_%EA%B0%80%EC%9D%B4%EB%93%9C(4)_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%B6%84%EC%84%9D(4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 06. 토픽 모델링(Topic Modeling) - 20 뉴스그룹\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Pvo-hSMl1hAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**토픽 모델링(Topic Modeling)** : 문서 집합에 숨어 있는 주제를 찾아내는 것\n",
        "\n",
        "-> 머신러닝 기반의 토픽 모델링 적용 : 숨겨진 주제를 효과적으로 표현할 수 있는 중심 단어를 함축적으로 추출\n",
        "\n",
        "* 자주 사용되는 기법은 **LSA(Latent Semantic Analysis)**와 **LDA(Latent Dirichlet Allocation)**\n",
        "\n",
        "* 사이킷런은 LDA 기반의 토픽 모델링을 **LatentDirichletAllocation** 클래스로 제공"
      ],
      "metadata": {
        "id": "DS21hm2p1muL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQJ0WgZFs_Ms",
        "outputId": "bd4009a0-5a19-43e2-d8bb-2ab65fa0249b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer Shape: (7862, 1000)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# 모토사이클, 야구, 그래픽스, 윈도우즈, 중동, 기독교, 전자공학, 의학 8개 주제를 추출.\n",
        "cats = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', 'comp.windows.x',\n",
        "        'talk.politics.mideast', 'soc.religion.christian', 'sci.electronics', 'sci.med']\n",
        "\n",
        "# 위에서 cats 변수로 기재된 카테고리만 추출. fetch_20newsgroups()의 categories에 cats 입력\n",
        "news_df = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'),\n",
        "                             categories=cats, random_state=0)   # categories 파라미터 통해 필요한 주제만 필터링해 추출\n",
        "\n",
        "# LDA는 Count기반의 벡터화만 적용합니다.\n",
        "count_vect = CountVectorizer(max_df=0.95, max_features=1000, min_df=2, stop_words='english',\n",
        "                             ngram_range=(1,2))      # max_features로 word 피처의 개수 제한\n",
        "feat_vect = count_vect.fit_transform(news_df.data)\n",
        "print('CountVectorizer Shape:', feat_vect.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> CountVectorizer 객체 변수인 feat_vect 모두 7862개의 문서가 1000개의 피처로 구성된 행렬 데이터"
      ],
      "metadata": {
        "id": "dm5JCdg-4ZPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 피처 벡터화된 데이터 세트 기반으로 LDA 토픽 모델링 수행 - LatentDirichletAllocation 클래스\n",
        "  + **n_components**파라미터 이용해 토픽 개수 조정"
      ],
      "metadata": {
        "id": "dgGe8UEo4gFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda = LatentDirichletAllocation(n_components=8, random_state=0)\n",
        "lda.fit(feat_vect)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "yDXSbbHW4sS5",
        "outputId": "87cb28e6-e8bd-4d1b-c347-9d38ccb7c8e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(n_components=8, random_state=0)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=8, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=8, random_state=0)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **components_** 속성값 가지게 돼 : 개별 토픽별로 각 word 피처가 얼마나 많이 그 토픽에 할당됐는지에 대한 수치\n",
        "  + 높은 값일수록 해당 word 피처는 그 토픽의 중심 word가 된다"
      ],
      "metadata": {
        "id": "1lVDV9IT4yc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(lda.components_.shape)\n",
        "lda.components_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDjop_Fw5COK",
        "outputId": "4c423877-2cfc-4f49-c22d-30f041584ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 1000)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.60992018e+01, 1.35626798e+02, 2.15751867e+01, ...,\n",
              "        3.02911688e+01, 8.66830093e+01, 6.79285199e+01],\n",
              "       [1.25199920e-01, 1.44401815e+01, 1.25045596e-01, ...,\n",
              "        1.81506995e+02, 1.25097844e-01, 9.39593286e+01],\n",
              "       [3.34762663e+02, 1.25176265e-01, 1.46743299e+02, ...,\n",
              "        1.25105772e-01, 3.63689741e+01, 1.25025218e-01],\n",
              "       ...,\n",
              "       [3.60204965e+01, 2.08640688e+01, 4.29606813e+00, ...,\n",
              "        1.45056650e+01, 8.33854413e+00, 1.55690009e+01],\n",
              "       [1.25128711e-01, 1.25247756e-01, 1.25005143e-01, ...,\n",
              "        9.17278769e+01, 1.25177668e-01, 3.74575887e+01],\n",
              "       [5.49258690e+01, 4.47009532e+00, 9.88524814e+00, ...,\n",
              "        4.87048440e+01, 1.25034678e-01, 1.25074632e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> 8개의 토픽별로 1000개의 word 피처가 해당 토픽별로 연관도 값 가지고 있다\n",
        "\n",
        "=> components_array의 0번째 row, 10번째 col에 있는 값 : Topic #0에 대해서 피처 벡터화된 행렬에서 10번째 칼럼에 해당하는 피처가 Topic #0에 연관되는 수치 값 가지고 있다"
      ],
      "metadata": {
        "id": "XSbap6i8AIAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 함수 만들어 각 토픽별로 연관도가 높은 순으로 Word 나열"
      ],
      "metadata": {
        "id": "T3EV3_YlAdJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_topics(model, feature_names, no_top_words):\n",
        "  for topic_index, topic in enumerate(model.components_):\n",
        "    print('Topic #', topic_index)\n",
        "\n",
        "    # components_array에서 가장 값이 큰 순으로 정렬했을 때, 그 값의 array 인덱스를 반환.\n",
        "    topic_word_indexes = topic.argsort()[::-1]\n",
        "    top_indexes = topic_word_indexes[:no_top_words]\n",
        "\n",
        "    # top_indexes대상인 인덱스별로 feature_names에 해당하는 word feature 추출 후 join으로 concat\n",
        "    feature_concat = ' '.join([feature_names[i] for i in top_indexes])\n",
        "    print(feature_concat)\n",
        "\n",
        "# CountVectorizer객체 내의 전체 word의 명칭을 get_feautres_names()를 통해 추출\n",
        "feature_names = count_vect.get_feature_names_out()\n",
        "\n",
        "# 토픽별 가장 연관도가 높은 word를 15개만 추출\n",
        "display_topics(lda, feature_names, 15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D4tvM5JAgeI",
        "outputId": "be01b644-1431-4d5e-f554-fe34fb27f1aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic # 0\n",
            "year 10 game medical health team 12 20 disease cancer 1993 games years patients good\n",
            "Topic # 1\n",
            "don just like know people said think time ve didn right going say ll way\n",
            "Topic # 2\n",
            "image file jpeg program gif images output format files color entry 00 use bit 03\n",
            "Topic # 3\n",
            "like know don think use does just good time book read information people used post\n",
            "Topic # 4\n",
            "armenian israel armenians jews turkish people israeli jewish government war dos dos turkey arab armenia 000\n",
            "Topic # 5\n",
            "edu com available graphics ftp data pub motif mail widget software mit information version sun\n",
            "Topic # 6\n",
            "god people jesus church believe christ does christian say think christians bible faith sin life\n",
            "Topic # 7\n",
            "use dos thanks windows using window does display help like problem server need know run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Topic #0의 경우 명확하지 않고 일반적인 단어, #1의 경우 컴퓨터 그래픽스 영역, #2는 기독교, #3는 의학, #4는 윈도우 운영체제, #5는 일반적인 단어, #6은 중동 분쟁 등, #7은 애매하지만 윈도우 운영체제 관련 일부 추출 / 특히 모토사이클, 야구 주제의 경우 명확한 주제어 추출x"
      ],
      "metadata": {
        "id": "p-vktXj0Bmtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 08. 문서 유사도\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "PcfRV8DFCAPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문서 유사도 측정 방법 - 코사인 유사도\n",
        "\n",
        "문서와 문서 간의 유사도 비교 -> **코사인 유사도** 사용\n",
        "\n",
        ": 두 벡터 사이의 사잇각을 구해서 벡터의 상호 방향성이 얼마나 유사한지 수치로 적용한 것"
      ],
      "metadata": {
        "id": "3L56HNE6CCf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 두 벡터 사잇각\n",
        "\n",
        "유사도 cos : 두 벡터의 내적을 총 벡터 크기의 합으로 나눈 것\n",
        "\n",
        "* 코사인 유사도가 문서의 유사도 비교에 가장 많이 사용되는 이유\n",
        "\n",
        "1. 무서를 피처 벡터화 변환하면 차원이 매우 많은 희소 행렬이 되기 쉽다 -> 희소 행렬 기반에서 문서와 문서 벡터 간의 크기에 기반한 유사도 지표는 정확도 떨어지기 쉽다\n",
        "\n",
        "2. 문서가 매우 긴 경우 단어의 빈도수도 더 많을 것이기 때문에 이러한 빈도수에만 기반해서는 공정한 비교할 수 없다"
      ],
      "metadata": {
        "id": "KbBLi7vwDSr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cos_similarity() 함수 작성\n",
        "import numpy as np\n",
        "\n",
        "def cos_similarity(v1, v2):\n",
        "  dot_product = np.dot(v1, v2)\n",
        "  l2_norm = (np.sqrt(sum(np.square(v1))) * np.sqrt(sum(np.square(v2))))\n",
        "  similarity = dot_product / l2_norm\n",
        "\n",
        "  return similarity"
      ],
      "metadata": {
        "id": "G8hwtCLkGV2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이 문서를 TF-IDF로 벡터화된 행렬로 변환\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "doc_list = ['if you take the blue pill, the story ends',\n",
        "            'if you take the red pill, you stay in Wonderland',\n",
        "            'if you take the red pill, I show you how deep the rabbit hole goes']\n",
        "\n",
        "tfidf_vect_simple = TfidfVectorizer()\n",
        "feature_vect_simple = tfidf_vect_simple.fit_transform(doc_list)\n",
        "print(feature_vect_simple.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9ouGudPGu06",
        "outputId": "7598eedd-d0be-4598-bf20-a18ec0f8d4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TfidfVectorizer로 transform()한 결과는 희소 행렬이므로 밀집 행렬로 변환.\n",
        "feature_vect_dense = feature_vect_simple.todense()\n",
        "\n",
        "# 첫 번째 문장과 두 번째 문장의 피처 벡터 추출\n",
        "vect1 = np.array(feature_vect_dense[0]).reshape(-1,)\n",
        "vect2 = np.array(feature_vect_dense[1]).reshape(-1,)\n",
        "\n",
        "# 첫 번째 문장과 두 번째 문장의 피처 벡터로 두 개 문장의 코사인 유사도 추출\n",
        "similarity_simple = cos_similarity(vect1, vect2)\n",
        "print('문장 1, 문장 2 Cosine 유사도: {0:.3f}'.format(similarity_simple))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B93OzSklH09w",
        "outputId": "13cabbd7-969d-44e4-c5ea-701ca4a16f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 1, 문장 2 Cosine 유사도: 0.402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫 번째 문장과 세 번째 문장의 유사도도 측정\n",
        "vect1 = np.array(feature_vect_dense[0]).reshape(-1,)\n",
        "vect3 = np.array(feature_vect_dense[2]).reshape(-1,)\n",
        "similarity_simple = cos_similarity(vect1, vect3)\n",
        "print('문장 1, 문장 3 Cosine 유사도: {0:.3f}'.format(similarity_simple))\n",
        "\n",
        "# 두 번째 문장과 세 번째 문장의 유사도도 측정\n",
        "vect2 = np.array(feature_vect_dense[1]).reshape(-1,)\n",
        "vect3 = np.array(feature_vect_dense[2]).reshape(-1,)\n",
        "similarity_simple = cos_similarity(vect2, vect3)\n",
        "print('문장 2, 문장 3 Cosine 유사도: {0:.3f}'.format(similarity_simple))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgxVg4VgImk8",
        "outputId": "89187f77-3bfb-4a13-adfe-cd020953c0b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 1, 문장 3 Cosine 유사도: 0.404\n",
            "문장 2, 문장 3 Cosine 유사도: 0.456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 사이킷런은 코사인 유사도를 측정하기 위해 **sklearn.metrics.pairwise.cosine_similarity** API 제공\n",
        "  + 두 개의 입력 파라미터 : 비교 기준이 되는 문서의 피처 행렬, 비교되는 문서의 피처 행렬\n",
        "  + 희소 행렬, 밀집 행렬 모두 가능, 행렬 또는 배열 모두 가능\n",
        "  + 별도의 변환 작업이 필요없다"
      ],
      "metadata": {
        "id": "pvKsUa7TJOSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫 번째 문서와 비교해 자신 문서인 첫 번째 문서, 그리고 두 번째, 세 번째 문서의 유사도 측정\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_simple_pair = cosine_similarity(feature_vect_simple[0], feature_vect_simple)\n",
        "print(similarity_simple_pair)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2do9QhCJvev",
        "outputId": "35bb1856-31e2-42fa-aa16-7ae7f95c14bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.40207758 0.40425045]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> 1이라는 값이 거슬린다면 feature_vect[1:] 이용해 비교 기준 문서 제외"
      ],
      "metadata": {
        "id": "RtGCHLyXKQy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_simple_pair = cosine_similarity(feature_vect_simple[0], feature_vect_simple[1:])\n",
        "print(similarity_simple_pair)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rpAG3vTKWjs",
        "outputId": "244fc52a-3c82-4681-d57a-3aa6fe0c345e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.40207758 0.40425045]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 쌍으로(pair) 코사인 유사도 값 제공\n",
        "  + 1번째 문서와 2,3번쨰 문서의 코사인 유사도, 2번째 문서와 1,3번째 문서의 코사인 유사도, 3번째 문서와 1,2번째 문서의 코사인 유사도를 ndarray 형태로 제공"
      ],
      "metadata": {
        "id": "AHi0j6DMKmnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_simple_pair = cosine_similarity(feature_vect_simple, feature_vect_simple)\n",
        "print(similarity_simple_pair)\n",
        "print('shape:', similarity_simple_pair.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBwUwkemKzL4",
        "outputId": "4bbfbc56-b7f5-4391-9221-602bbee555b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.40207758 0.40425045]\n",
            " [0.40207758 1.         0.45647296]\n",
            " [0.40425045 0.45647296 1.        ]]\n",
            "shape: (3, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Opinion Review 데이터 세트를 이용한 문서 유사도 측정"
      ],
      "metadata": {
        "id": "8tVmQ0RDLHxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 문서 군집화 적용"
      ],
      "metadata": {
        "id": "r7nroXicLNlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "# 단어 원형 추출 함수\n",
        "lemmar = WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "    return [lemmar.lemmatize(token) for token in tokens]\n",
        "\n",
        "# 특수 문자 사전 생성: {33: None ...}\n",
        "# ord(): 아스키 코드 생성\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "# 특수 문자 제거 및 단어 원형 추출\n",
        "def LemNormalize(text):\n",
        "    # 텍스트 소문자 변경 후 특수 문자 제거\n",
        "    text_new = text.lower().translate(remove_punct_dict)\n",
        "\n",
        "    # 단어 토큰화\n",
        "    word_tokens = nltk.word_tokenize(text_new)\n",
        "\n",
        "    # 단어 원형 추출\n",
        "    return LemTokens(word_tokens)"
      ],
      "metadata": {
        "id": "dCTHhiG-Lc2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob, os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# 경로 이동\n",
        "%cd /content/drive/MyDrive/data/topics\n",
        "all_files=glob.glob(os.path.join('*.data'))\n",
        "filename_list=[]\n",
        "opinion_text=[]\n",
        "\n",
        "for file_ in all_files:\n",
        "  df=pd.read_table(file_, index_col=None, header=0, encoding='latin1')\n",
        "  filename_=file_.split('\\\\')[-1]\n",
        "  filename=filename_.split('.')[0]\n",
        "  filename_list.append(filename)\n",
        "  opinion_text.append(df.to_string())\n",
        "\n",
        "document_df=pd.DataFrame({'filename':filename_list, 'opinion_text':opinion_text})\n",
        "\n",
        "tfidf_vect=TfidfVectorizer(tokenizer=LemNormalize, stop_words='english',\n",
        "                           ngram_range=(1,2), min_df=0.05, max_df=0.85)\n",
        "feature_vect=tfidf_vect.fit_transform(document_df['opinion_text'])\n",
        "km_cluster=KMeans(n_clusters=3, max_iter=10000, random_state=0)\n",
        "km_cluster.fit(feature_vect)\n",
        "cluster_label=km_cluster.labels_\n",
        "cluster_centers=km_cluster.cluster_centers_\n",
        "document_df['cluster_label']=cluster_label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-fF4EweMOPM",
        "outputId": "159c6927-037f-4258-d3a4-886fc54c9af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/data/topics'\n",
            "/content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-272eeb50e59f>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m tfidf_vect=TfidfVectorizer(tokenizer=LemNormalize, stop_words='english', \n\u001b[1;32m     25\u001b[0m                            ngram_range=(1,2), min_df=0.05, max_df=0.85)\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mfeature_vect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfidf_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'opinion_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mkm_cluster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mkm_cluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2131\u001b[0m             \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m         )\n\u001b[0;32m-> 2133\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1386\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1292\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1295\u001b[0m                     \u001b[0;34m\"empty vocabulary; perhaps the documents only contain stop words\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 호텔을 주제로 군집화된 문서 이용해 특정 문서와 다른 문서 간의 유사도 알아보기\n",
        "  + 문서를 피처 벡터화해 변환하면 문서 내 단어에 출현 빈도와 같은 값을 부여해 각 문서가 단어 피처의 값으로 벡터화돼 -> cosine_similarity() 이용해 상호 비교해 유사도 확인"
      ],
      "metadata": {
        "id": "uiMLvSU-jBDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# cluster_label=1인 데이터는 호텔로 군집화된 데이터임. DataFrame에서 해당 인덱스를 추출\n",
        "hotel_indexes = document_df[document_df['cluster_label']==1].index\n",
        "print('호텔로 군집화 된 문서들의 DataFrame Index:', hotel_indexes)\n",
        "\n",
        "# 호텔로 군집화된 데이터 중 첫 번째 문서를 추출해 파일명 표시.\n",
        "comparison_docname = document_df.iloc[hotel_indexes[0]]['filename']\n",
        "print('##### 비교 기준 문서명 ', comparison_docname, ' 와 타 문서 유사도######')\n",
        "\n",
        "''' document_df에서 추출한 Index 객체를 feature_vect로 입력해 호텔 군집화된 feature_vect 추출\n",
        "이를 이용해 호텔로 군집화된 문서 중 첫 번째 문서와 다른 문서 간의 코사인 유사도 측정.'''\n",
        "similarity_pair = cosine_similarity(feature_vect[hotel_indexes[0]], feature_vect[hotel_indexes])\n",
        "print(similarity_pair)"
      ],
      "metadata": {
        "id": "Yi2czUfYl9bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 첫 번째 문서와 다른 문서 간에 유사도가 높은 순으로 정렬하고 시각화\n",
        "  + cosine_similarity()는 쌍 형태의 ndarray 반환 -> 판다스 인덱스로 이용하기 위해 reshape(-1)로 차원 변경"
      ],
      "metadata": {
        "id": "BVliCqHxn7ID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# 첫 번째 문서와 타 문서 간 유사도가 큰 순으로 정렬한 인덱스를 추출하되 자기 자신은 제외.\n",
        "sorted_index = similarity_pair.argsort()[:, ::-1]\n",
        "sorted_index = sorted_index[:, 1:]\n",
        "\n",
        "# 유사도가 큰 순으로 hotel_indexes를 추출해 재정렬\n",
        "hotel_sorted_indexes = hotel_indexes[sorted_index.reshape(-1)]\n",
        "\n",
        "# 유사도가 큰 순으로 유사도 값을 재정렬하되 자기 자신은 제외\n",
        "hotel_1_sim_value = np.sort(similarity_pair.reshape(-1))[::-1]\n",
        "hotel_1_sim_value = hotel_1_sim_value[1:]\n",
        "\n",
        "# 유사도가 큰 순으로 정렬된 인덱스와 유사도 값을 이용해 파일명과 유사도 값을 막대 그래프로 시각화\n",
        "hotel_1_sim_df = pd.DataFrame()\n",
        "hotel_1_sim_df['filename'] = document_df.iloc[hotel_sorted_indexes]['filename']\n",
        "hotel_1_sim_df['similarity'] = hotel_1_sim_value\n",
        "\n",
        "sns.barplot(x='similarity', y='filename', data=hotel_1_sim_df)\n",
        "plt.title(comparison_docname)"
      ],
      "metadata": {
        "id": "0RLVepIxoAE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> 첫 번째 문서인 샌프란시스코의 베스트 웨스턴 호텔 화장실 리뷰(Best Western Hotel Bathroom Review)인 bathroom_bestwestern_hotel_sfo와 가장 비슷한 문서는 room_holidaty_inn_london -> 약 0.572의 코사인 유사도 값"
      ],
      "metadata": {
        "id": "15EZ8F41pIGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09. 한글 텍스트 처리 - 네이버 영화 평점 감성 분석\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hiWAtUyOpZ2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 한글 NLP 처리의 어려움\n",
        "\n",
        "일반적으로 한글 언어 처리는 영어 등의 라틴어 처리보다 어렵다\n",
        "\n",
        "주된 원인 - '띄어쓰기'와 '다양한 조사'\n",
        "\n",
        "1. 한글은 띄어쓰기를 잘못하면 의미가 왜곡되어 전달\n",
        "2. 조사는 주어나 목적어를 위해 추가. 워낙 경우의 수가 많아 -> 전처리 시 제거하기가 까다로워"
      ],
      "metadata": {
        "id": "Pk_tuskAScs7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KoNLPy 소개\n",
        "\n",
        "**KoNLPy** : 파이썬의 대표적인 한글 형태소 패키지\n",
        "\n",
        "* 형태소 : 단어로서 의미를 가지는 최소 단위\n",
        "  + 형태소 분석 : 말뭉치를 이러한 형태소 어근 단위로 쪼개고 각 형태소에 품사 태깅을 부착하는 작업\n",
        "\n",
        "* KoNLPy : 기존의 C/C++, Java로 잘 만들어진 한글 형태소 엔진을 파이썬 래퍼(Wrapper) 기반으로 재작성한 패키지\n",
        "  + 꼬꼬마(Kkma), 한나눔(Hannanum), Komoran, 은전한닢 프로젝트(Mecab), Twitter와 같이 5개의 형태소 분석 모듈 모두 사용 가능\n",
        "  + 뛰어난 형태소 분석으로 인정받고 있는 Mecab의 경우 윈도우 환경에서는 구동x."
      ],
      "metadata": {
        "id": "CWYusr3pS8D2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install JPype1-1.4.0-cp310-cp310-win_amd64.whl"
      ],
      "metadata": {
        "id": "4kLjdw4oaOXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "id": "IssXVI5AeQVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 로딩"
      ],
      "metadata": {
        "id": "M6k1iRA1gOzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/data/nsmc/ratings_train.txt', sep='\\t')\n",
        "train_df.head(3)"
      ],
      "metadata": {
        "id": "SgUZQ5YsgTXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 학습 데이터 세트의 0과 1의 Label 값 비율\n",
        "  + 1이 긍정, 0이 부정 감성"
      ],
      "metadata": {
        "id": "f8l_38ctghAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['label'].value_counts()"
      ],
      "metadata": {
        "id": "fQyDjGj9gmCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> 0과 1의 비율이 어느 한쪽으로 치우치지 않고 균등한 분포 나타냄"
      ],
      "metadata": {
        "id": "SbQsWFJ-gyfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* train_df의 경우 리뷰 텍스트를 가지는 'documnet' 칼럼에 Null 일부 존재해 공백으로 변환\n",
        "* 문자가 아닌 숫자의 경우 단어적인 의미로 부족해 파이썬 정규 표현식 모듈인 re 이용해 공백으로 변환\n",
        "* 테스트 데이터 세트의 경우도 동일한 데이터 가공 수행"
      ],
      "metadata": {
        "id": "IfRF82AKg3eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "train_df = train_df.fillna(' ')\n",
        "# 정규 표현식을 이용해 숫자를 공백으로 변경(정규 표현식으로 \\d는 숫자를 의미함.)\n",
        "train_df['document'] = train_df['document'].apply(lambda x : re.sub(r\"\\d+\", \" \", x))\n",
        "\n",
        "# 테스트 데이터 세트를 로딩하고 동일하게 Null 및 숫자를 공백으로 변환\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/data/nsmc/ratings_test.txt', sep='\\t')\n",
        "test_df = test_df.fillna(' ')\n",
        "test_df['document'] = test_df['document'].apply(lambda x : re.sub(r\"\\d+\", \" \", x))\n",
        "\n",
        "# id 칼럼 삭제 수행\n",
        "train_df.drop('id', axis=1, inplace=True)\n",
        "test_df.drop('id', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "ySKLd_mthKJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 각 문장을 한글 형태소 분석을 통해 형태소 단어로 토큰화\n",
        "* 한글 형태소 엔진은 SNS 분석에 적합한 **Twitter 클래스** 이용\n",
        "  + Twitter 객체의 **morphs()** 메서드 이용 : 입력 인자로 들어온 문장을 형태소 단어 형태로 토큰화해 list 객체로 반환\n",
        "* 문장을 형태소 단어 형태로 반환하는 별도의 tokenizer 함수 생성"
      ],
      "metadata": {
        "id": "7dzBYb5Ah_xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Twitter\n",
        "\n",
        "# \"Twitter\" has changed to \"Okt\"\n",
        "twitter = Twitter()\n",
        "def tw_tokenizer(text):\n",
        "  # 입력 인자로 들어온 텍스트를 형태소 단어로 토큰화해 리스트 형태로 반환\n",
        "  tokens_ko = twitter.morphs(text)\n",
        "  return tokens_ko"
      ],
      "metadata": {
        "id": "LfUR6_Bbh8wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 사이킷런의 TfidfVectorizer 이용해 TF-IDF 피처 모델 생성"
      ],
      "metadata": {
        "id": "VOOntnQZVjZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Twitter 객체의 morphs() 객체를 이용한 tokenizer를 사용. ngram_range는 (1,2)\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=tw_tokenizer, ngram_range=(1,2), min_df=3, max_df=0.9)\n",
        "tfidf_vect.fit(train_df['document'])\n",
        "tfidf_matrix_train = tfidf_vect.transform(train_df['document'])"
      ],
      "metadata": {
        "id": "jqnXUhxsVn3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 로지스틱 회귀 이용해 분류 기반의 감성 분석 수행"
      ],
      "metadata": {
        "id": "UhR1pSCKWIz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 로지스틱 회귀를 이용해 감성 분석 분류 수행.\n",
        "lg_clf = LogisticRegression(random_state=0)\n",
        "\n",
        "# 파라미터 C 최적화를 위해 GridSearchCV를 이용.\n",
        "params = {'C' : [1, 3.5, 4.5, 5.5, 10]}\n",
        "grid_cv = GridSearchCV(lg_clf, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
        "grid_cv.fit(tfidf_matrix_train, train_df['label'])\n",
        "print(grid_cv.best_params_, round(grid_cv.best_score_, 4))"
      ],
      "metadata": {
        "id": "1QJp8O9wWLwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 테스트 세트 이용해 최종 감성 분석 예측 수행\n",
        "  + 테스트 세트 이용해 예측할 때는 **학습할 때 적용한 TfidfVectorizer 그대로 사용**해야한다\n",
        "  + 그래야만 학습 시 설정된 TfidfVectorizer의 피처 개수와 테스트 데이터를 TfidfVectorizer로 변환할 피처 개수가 같아진다"
      ],
      "metadata": {
        "id": "o_9xVUKDWuoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 학습 데이터를 적용한 TfidfVectorizer를 이용해 테스트 데이터를 TF-IDF 값으로 피처 변환함.\n",
        "tfidf_matrix_test = tfidf_vect.transform(test_df['document'])\n",
        "\n",
        "# classifier는 GridSearchCV에서 최적 파라미터로 학습된 classifier를 그대로 이용\n",
        "best_estimator = grid_cv.best_estimator_\n",
        "preds = best_estimator.predict(tfidf_matrix_test)\n",
        "\n",
        "print('Logistic Regression 정확도: ', accuracy_score(test_df['label'], preds))"
      ],
      "metadata": {
        "id": "lNrJ70-KW7Vw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}